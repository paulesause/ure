{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a45adca",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82318c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b62f4d",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Dowload spacy tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431e2f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b9fca",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8056ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version from kaggle hub\n",
    "path = kagglehub.dataset_download(\"daishinkan002/new-york-times-relation-extraction-dataset\")\n",
    "train_path = f\"{path}/dataset/train.json\"\n",
    "test_path = f\"{path}/dataset/test.json\"\n",
    "valid_path = f\"{path}/dataset/valid.json\"\n",
    "\n",
    "df_train = pd.read_json(train_path, lines=True)\n",
    "df_valid = pd.read_json(valid_path, lines=True)\n",
    "df_test = pd.read_json(test_path, lines=True)\n",
    "\n",
    "# create subsets for testing\n",
    "df_train = df_train.sample(n=100)\n",
    "df_valid = df_valid.sample(n=100)\n",
    "df_test = df_test.sample(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe137a28",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6940aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dep_path(doc, span1, span2):\n",
    "    t1 = span1.root\n",
    "    t2 = span2.root\n",
    "\n",
    "    path1 = []\n",
    "    path2 = []\n",
    "\n",
    "    p = t1\n",
    "    while p is not None:\n",
    "        path1.append(p)\n",
    "        p = p.head if p != p.head else None\n",
    "\n",
    "    q = t2\n",
    "    while q is not None:\n",
    "        path2.append(q)\n",
    "        q = q.head if q != q.head else None\n",
    "\n",
    "    lca = next(t for t in path1 if t in path2)\n",
    "\n",
    "    up = []\n",
    "    cur = t1\n",
    "    while cur != lca:\n",
    "        up.append(f\"<-{cur.dep_}\")\n",
    "        cur = cur.head\n",
    "\n",
    "    rev = []\n",
    "    cur = t2\n",
    "    while cur != lca:\n",
    "        rev.append(f\"->{cur.dep_}\")\n",
    "        cur = cur.head\n",
    "    down = list(reversed(rev))\n",
    "\n",
    "    return \"\".join(up + down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f5791b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_span(doc, entity):\n",
    "                start_char = doc.text.find(entity)\n",
    "                if start_char == -1:\n",
    "                    return None\n",
    "                end_char = start_char + len(entity)\n",
    "                return doc.char_span(start_char, end_char, alignment_mode=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9ebac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_path_tokens(doc, t1, t2):\n",
    "    \"\"\"Return the list of tokens along the dependency path.\"\"\"\n",
    "    # Path from t1 → root\n",
    "    path1 = []\n",
    "    x = t1\n",
    "    while x is not None:\n",
    "        path1.append(x)\n",
    "        if x == x.head:\n",
    "            break\n",
    "        x = x.head\n",
    "\n",
    "    # Path from t2 → root\n",
    "    path2 = []\n",
    "    y = t2\n",
    "    while y is not None:\n",
    "        path2.append(y)\n",
    "        if y == y.head:\n",
    "            break\n",
    "        y = y.head\n",
    "\n",
    "    # Find LCA\n",
    "    lcas = set(path1) & set(path2)\n",
    "    if not lcas:\n",
    "        return []  # no path found\n",
    "    lca = next(t for t in path1 if t in lcas)\n",
    "\n",
    "    # Build path from t1 → LCA\n",
    "    up = []\n",
    "    cur = t1\n",
    "    while cur != lca:\n",
    "        up.append(cur)\n",
    "        cur = cur.head\n",
    "    up.append(lca)\n",
    "\n",
    "    # Build path from LCA → t2\n",
    "    down = []\n",
    "    cur = t2\n",
    "    tmp = []\n",
    "    while cur != lca:\n",
    "        tmp.append(cur)\n",
    "        cur = cur.head\n",
    "    down = list(reversed(tmp))\n",
    "\n",
    "    # Combined token path\n",
    "    return up + down\n",
    "\n",
    "\n",
    "def get_trigger(doc, dep_path_tokens):\n",
    "    for t in dep_path_tokens:\n",
    "        if t.pos_ in {\"NOUN\", \"VERB\"}:\n",
    "            return t.text\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efe6de3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deppath is NA\n",
      "New York Authorities Plan Civil Complaint Against A.I.G. and Two Former Officers The New York attorney general and the state insurance department are expected to file a civil complaint against the American International Group , its former chairman and chief executive , Maurice R. Greenberg , and its former chief financial officer , accusing them of accounting practices that allowed it to make its financial position look stronger than it actually was .\n",
      "deppath is NA\n",
      "Gene Pitney , the clean-cut crooner who became a teenage idol in the early 1960 's with hits like '' Town Without Pity , '' '' -LRB- The Man Who Shot -RRB- Liberty Valance '' and '' Only Love Can Break a Heart '' -- songs that showcased his keening tenor -- was found dead yesterday in a hotel in Cardiff , Wales , while on a tour of Britain .\n",
      "deppath is NA\n",
      "He is also the owner of the Glen Head Racquet Club in Glen Head , N.Y. Her mother retired as an occupational therapist at the Association for Children With Down Syndrome in Plainview , N.Y. The bridegroom , 31 , is a partner in Exton Capital Management , a hedge fund in New York .\n",
      "deppath is NA\n",
      "-- Ann Fredericks , New York , N.Y. A company that has provided this service since the 1950 's -LRB- and is apparently the only one still doing this -RRB- is Liberty Moving and Storage in Hauppauge , N.Y. Roberta Snair , the sales manager at Liberty , said it rented portable wooden wardrobes that are like old steamer trunks , large enough -- about 36 inches wide , 18 inches deep , and 5 feet high -- to hang clothes on hangers inside .\n",
      "deppath is NA\n",
      "Ms. Barton 's freshness was astonishing , and in a way historical , her expression the exact same persuasive mixture of steely comprehension and practiced innocence that one finds in Colette -LRB- that is Colette the French writer , not Colette the Paris boutique -RRB- . ''\n",
      "deppath is NA\n",
      "Next month , Marriott International will open its 20th Caribbean resort , the 122-room Courtyard by Marriott , Port of Spain -LRB- 868-627-5555 ; marriott.com -RRB- , in Trinidad .\n",
      "null triggger: 0\n",
      "deppath is NA\n",
      "Against that backdrop , Fred Siegel , a history professor at Cooper Union and a former unpaid campaign policy adviser to Rudolph W. Giuliani , has written '' The Prince of the City : Giuliani , New York and the Genius of American Life . ''\n",
      "deppath is NA\n",
      "In celebration of its 20th anniversary , Edward Villella 's Miami City Ballet has embarked on a short tour that has taken it to Virginia and the New York area -LRB- but not New York City -RRB- , with a footnote in Los Angeles in late June and early July .\n",
      "deppath is NA\n",
      "The Stowell-Russell team did indeed make Seattle one of the prime examples of the Balanchine diaspora , and Mr. Boal ended his season on Sunday with the last performance of a faithful restaging -LRB- Suzanne Farrell had a hand in the coaching -RRB- of Balanchine 's '' Jewels , '' which Pacific Northwest Ballet had never done in its entirety .\n",
      "null triggger: 0\n",
      "null triggger: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ToDo: some relationships appear douple in the ouptut data .txt!!\n",
    "\n",
    "def preprocess_kaggle_df(df) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Every setence of the NYT data set can possibly contain more than one relationships.\n",
    "    This methods s\n",
    "    \n",
    "    1.) plits them up and creates a pandas.Dataframe with one line for each every mentioned relations\n",
    "    and available corresponding relationship types. Relationships with no given corresponding relationship types are ommitted.\n",
    "\n",
    "    2.) Tokenize the sentence\n",
    "\n",
    "    3.) Create POS Tags\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    null_trigger = 0\n",
    "    \n",
    "    columns=[ 'deppath',\n",
    "        'entity_head',\n",
    "                     'entity_tail',\n",
    "                     'enttypes',\n",
    "                     'trigger', \n",
    "                     'fname',\n",
    "                     'tokenized_text',\n",
    "                     \"POS\",\n",
    "                     'relationship']\n",
    "    \n",
    "    df_structured = pd.DataFrame(columns=columns)\n",
    "\n",
    "    for row in df.itertuples():\n",
    "\n",
    "        # Tokize Sentence\n",
    "        raw_text = row.sentText\n",
    "        doc = nlp(raw_text)\n",
    "        tokens = [t.text for t in doc]\n",
    "        pos = [t.pos_ for t in doc]\n",
    "        \n",
    "        tokenized_text = tokens[0]\n",
    "        tokens.pop(0)\n",
    "        for t in tokens:\n",
    "            tokenized_text = tokenized_text + \" \" + t\n",
    "        \n",
    "        # Get POS taggs\n",
    "        pos_text = pos[0]\n",
    "        pos.pop(0)\n",
    "        for p in pos:\n",
    "            pos_text = pos_text + \" \" + p\n",
    "\n",
    "        for r in row.relationMentions:\n",
    "            # get entity head and tailk\n",
    "            entity_head = r['em1Text']\n",
    "            entity_tail = r['em2Text']\n",
    "\n",
    "            # get relationship\n",
    "            relationship = r[\"label\"]\n",
    "\n",
    "            # get entity types\n",
    "            type_head = None\n",
    "            type_tail = None\n",
    "            for e in row.entityMentions:\n",
    "                if e['text'] == entity_head:\n",
    "                    type_head = e['label']\n",
    "                if e['text'] == entity_tail:\n",
    "                    type_tail = e['label']\n",
    "\n",
    "            if not (type_head and type_tail):\n",
    "                continue\n",
    "\n",
    "            enttypes = type_head + \"-\" + type_tail\n",
    "\n",
    "            # create dependenthy path    \n",
    "            span1 = find_span(doc, entity_head)\n",
    "            span2 = find_span(doc, entity_tail)\n",
    "\n",
    "            if span1 is None or span2 is None:\n",
    "                deppath = \"NA\"\n",
    "                print(\"deppath is NA\")\n",
    "                print(raw_text)\n",
    "            else:\n",
    "                try:\n",
    "                    deppath = get_dep_path(doc, span1, span2)\n",
    "                except StopIteration:\n",
    "                    deppath = \"NA\"\n",
    "                    print(\"deppath is NA\")\n",
    "                    print(raw_text)\n",
    "\n",
    "            # get trigger\n",
    "            tokens = dep_path_tokens(doc, span1.root, span2.root)\n",
    "            trigger = get_trigger(doc, tokens)\n",
    "\n",
    "            # construct row for .txt\n",
    "            r_tmp = { 'deppath': deppath,\n",
    "                'entity_head': entity_head,\n",
    "                     'entity_tail': entity_tail,\n",
    "                     'enttypes': enttypes,\n",
    "                     'trigger': trigger, \n",
    "                     'fname': str(uuid.uuid4()),\n",
    "                     'tokenized_text': tokenized_text,\n",
    "                     \"POS\": pos_text,\n",
    "                     'relationship': relationship}\n",
    "\n",
    "            if not trigger:\n",
    "                continue\n",
    "            if deppath == \"NA\":\n",
    "                continue\n",
    "            if type_head and type_tail:\n",
    "                df_structured.loc[len(df_structured)] = r_tmp\n",
    "    \n",
    "    print(\"null triggger:\", null_trigger)\n",
    "    return df_structured\n",
    "\n",
    "\n",
    "\n",
    "train_prepocessed = preprocess_kaggle_df(df_train)\n",
    "valid_prepocessed = preprocess_kaggle_df(df_valid)\n",
    "test_prepocessed = preprocess_kaggle_df(df_test)\n",
    "\n",
    "\n",
    "\n",
    "def write_to_txt(df, path):\n",
    "    df.to_csv(path, sep='\\t', index=False, header=False)\n",
    "\n",
    "train_txt_path = \"../data/nyt/train.txt\"\n",
    "valid_txt_path = \"../data/nyt/dev.txt\"\n",
    "test_txt_path = \"../data/nyt/test.txt\"\n",
    "\n",
    "write_to_txt(train_prepocessed, train_txt_path)\n",
    "write_to_txt(valid_prepocessed, valid_txt_path)\n",
    "write_to_txt(test_prepocessed, test_txt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9277967",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "doc = nlp(\"1, 2, 3, 4, 5\")\n",
    "\n",
    "tokens = [t.text for t in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "914f6674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', ',', '3', ',', '4', ',', '5']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.pop(0)\n",
    "tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
